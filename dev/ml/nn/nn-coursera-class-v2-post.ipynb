{
 "metadata": {
  "name": "nn-coursera-class-v2-post"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Is really difficult be part of the data science / machine learning community and have not heard about Deep Neural Networks, everybody is talking about them. Is even harder for a person like me without a PhD and without a deep computer science or mathematics education to learn about them, because 1. Machine learning uses a quite heavy math and 2. There are no Neural Networks on sklearn. Libraries like [sklearn](http://scikit-learn.org/dev/index.html) hide all the stuff and let you use machine learning and get amazing results. But sometimes you need more, and also understanding how an algorithm is implemented can help you understand how to improve results. The learning curve is hard, you can easily spend hours on [DeepLearning.net](http://deeplearning.net/tutorial/) and not understand anything. But there is hope. \n",
      "\n",
      "I took Andrew's Ng [Machine Learning course](https://www.coursera.org/course/ml) on Coursera wanting to get a better undesrtanding of how the algorithms I use almost everyday work. I learned a lot of usefull tricks and learned a lot more about simple machine learning implementations. Is important to start with the easy stuff or you get overwhelmed easy and eventually give up. Hopefully in a few more weeks I would be able to understand two or three more words on [DeepLearning.net](http://deeplearning.net/tutorial/).\n",
      "\n",
      "The course is amazing, probably the best I have taken on Coursera (this was my 5th course). Amazing videos, amazing content, the course is quite hard but gives enough help to make it hard enough to NOT give up.\n",
      "\n",
      "The \"bad\" part is that the course uses Matlab/Octave. There is nothing like it for matrix stuff (specially Matlab) but Python is my to go language for almost everything so I decide to translate some Matlab code hoping to undestand more about the Neural Network implementation. And in the process obtain some reusable code for a neural network.\n",
      "\n",
      "The implementation follows the sklearn sintax: fit, predict, predict_proba. \n",
      "\n",
      "Also note that this is a fixed 1 hidden layer neural network, trained via back propagation. Basicly is the same Neural Network Andrew teached on the course but on python, nothing deep here."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Implementation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy import optimize\n",
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NN_1HL(object):\n",
      "    \n",
      "    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size=25, opti_method='TNC', maxiter=500):\n",
      "        self.reg_lambda = reg_lambda\n",
      "        self.epsilon_init = epsilon_init\n",
      "        self.hidden_layer_size = hidden_layer_size\n",
      "        self.activation_func = self.sigmoid\n",
      "        self.activation_func_prime = self.sigmoid_prime\n",
      "        self.method = opti_method\n",
      "        self.maxiter = maxiter\n",
      "    \n",
      "    def sigmoid(self, z):\n",
      "        return 1 / (1 + np.exp(-z))\n",
      "    \n",
      "    def sigmoid_prime(self, z):\n",
      "        sig = self.sigmoid(z)\n",
      "        return sig * (1 - sig)\n",
      "    \n",
      "    def sumsqr(self, a):\n",
      "        return np.sum(a ** 2)\n",
      "    \n",
      "    def rand_init(self, l_in, l_out):\n",
      "        return np.random.rand(l_out, l_in + 1) * 2 * self.epsilon_init - self.epsilon_init\n",
      "    \n",
      "    def pack_thetas(self, t1, t2):\n",
      "        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))\n",
      "    \n",
      "    def unpack_thetas(self, thetas, input_layer_size, hidden_layer_size, num_labels):\n",
      "        t1_start = 0\n",
      "        t1_end = hidden_layer_size * (input_layer_size + 1)\n",
      "        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))\n",
      "        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))\n",
      "        return t1, t2\n",
      "    \n",
      "    def _forward(self, X, t1, t2):\n",
      "        m = X.shape[0]\n",
      "        ones = None\n",
      "        if len(X.shape) == 1:\n",
      "            ones = np.array(1).reshape(1,)\n",
      "        else:\n",
      "            ones = np.ones(m).reshape(m,1)\n",
      "        \n",
      "        # Input layer\n",
      "        a1 = np.hstack((ones, X))\n",
      "        \n",
      "        # Hidden Layer\n",
      "        z2 = np.dot(t1, a1.T)\n",
      "        a2 = self.activation_func(z2)\n",
      "        a2 = np.hstack((ones, a2.T))\n",
      "        \n",
      "        # Output layer\n",
      "        z3 = np.dot(t2, a2.T)\n",
      "        a3 = self.activation_func(z3)\n",
      "        return a1, z2, a2, z3, a3\n",
      "    \n",
      "    def function(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
      "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
      "        \n",
      "        m = X.shape[0]\n",
      "        Y = np.eye(num_labels)[y]\n",
      "        \n",
      "        _, _, _, _, h = self._forward(X, t1, t2)\n",
      "        costPositive = -Y * np.log(h).T\n",
      "        costNegative = (1 - Y) * np.log(1 - h).T\n",
      "        cost = costPositive - costNegative\n",
      "        J = np.sum(cost) / m\n",
      "        \n",
      "        if reg_lambda != 0:\n",
      "            t1f = t1[:, 1:]\n",
      "            t2f = t2[:, 1:]\n",
      "            reg = (self.reg_lambda / (2 * m)) * (self.sumsqr(t1f) + self.sumsqr(t2f))\n",
      "            J = J + reg\n",
      "        return J\n",
      "        \n",
      "    def function_prime(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
      "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
      "        \n",
      "        m = X.shape[0]\n",
      "        t1f = t1[:, 1:]\n",
      "        t2f = t2[:, 1:]\n",
      "        Y = np.eye(num_labels)[y]\n",
      "        \n",
      "        Delta1, Delta2 = 0, 0\n",
      "        for i, row in enumerate(X):\n",
      "            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)\n",
      "            \n",
      "            # Backprop\n",
      "            d3 = a3 - Y[i, :].T\n",
      "            d2 = np.dot(t2f.T, d3) * self.activation_func_prime(z2)\n",
      "            \n",
      "            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])\n",
      "            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])\n",
      "            \n",
      "        Theta1_grad = (1 / m) * Delta1\n",
      "        Theta2_grad = (1 / m) * Delta2\n",
      "        \n",
      "        if reg_lambda != 0:\n",
      "            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f\n",
      "            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f\n",
      "        \n",
      "        return self.pack_thetas(Theta1_grad, Theta2_grad)\n",
      "    \n",
      "    def fit(self, X, y):\n",
      "        num_features = X.shape[0]\n",
      "        input_layer_size = X.shape[1]\n",
      "        num_labels = len(set(y))\n",
      "        \n",
      "        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)\n",
      "        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)\n",
      "        thetas0 = self.pack_thetas(theta1_0, theta2_0)\n",
      "        \n",
      "        options = {'maxiter': self.maxiter}\n",
      "        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, \n",
      "                                 args=(input_layer_size, self.hidden_layer_size, num_labels, X, y, 0), options=options)\n",
      "        \n",
      "        self.t1, self.t2 = self.unpack_thetas(_res.x, input_layer_size, self.hidden_layer_size, num_labels)\n",
      "    \n",
      "    def predict(self, X):\n",
      "        return self.predict_proba(X).argmax(0)\n",
      "    \n",
      "    def predict_proba(self, X):\n",
      "        _, _, _, _, h = self._forward(X, self.t1, self.t2)\n",
      "        return h"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "How good is it?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's test it with the allways good iris dataset using a single split for cross validation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.datasets as datasets\n",
      "from sklearn import cross_validation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN_1HL()\n",
      "nn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:64: RuntimeWarning: divide by zero encountered in log\n",
        "-c:64: RuntimeWarning: invalid value encountered in multiply\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import accuracy_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy_score(y_test, nn.predict(X_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "0.96666666666666667"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Nice 96% on the cross validation dataset! \n",
      "\n",
      "But wait the iris dataset is quite simple how well other algorithms do? What about my other favorite algorithm: Random Forest?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rfc = RandomForestClassifier(n_estimators=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rfc.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "RandomForestClassifier(bootstrap=True, compute_importances=False,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            min_density=0.1, min_samples_leaf=1, min_samples_split=2,\n",
        "            n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
        "            verbose=0)"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy_score(y_test, rfc.predict(X_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "0.96666666666666667"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "OK, the same result that is good, sometimes you get 96% or 98% because of the random state of the algorithms, but is fine."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Really, How good is it?!!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets try again with a more complex example, the same example of the coursera course: recognize hand writter numbers [5000 rows, 400 dimmensions]."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.io import loadmat\n",
      "data = loadmat('ex3data1.mat')\n",
      "X, y = data['X'], data['y']\n",
      "y = y.reshape(X.shape[0], )\n",
      "y = y - 1  # Fix notation # TODO: Automaticlly fix that on the class"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN_1HL(maxiter=200)\n",
      "nn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy_score(y_test, nn.predict(X_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "0.91349999999999998"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rfc = RandomForestClassifier(n_estimators=50)\n",
      "rfc.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "RandomForestClassifier(bootstrap=True, compute_importances=False,\n",
        "            criterion='gini', max_depth=None, max_features='auto',\n",
        "            min_density=0.1, min_samples_leaf=1, min_samples_split=2,\n",
        "            n_estimators=50, n_jobs=1, oob_score=False, random_state=None,\n",
        "            verbose=0)"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy_score(y_test, rfc.predict(X_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "0.91600000000000004"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Conclusions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Basiclly the same result for both algorithms. Lets decide a winner by the time they took to train.\n",
      "\n",
      "It took a long time to train the neural network on the second example, don't have and exact number but ... ok let's get that number (god I love [blogging with ipython notebooks](http://danielfrg.github.io/blog/2013/03/08/pelican-ipython-notebook-plugin/)):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN_1HL(maxiter=200)\n",
      "%timeit nn.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 119 s per loop\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, it took 119s, thats it [1.98 minutes](https://www.google.com/search?q=119s+in+minues&oq=119s+in+minues&aqs=chrome.0.57j0.3758j0&sourceid=chrome&ie=UTF-8) using a maximum number of iterations of 200, more will probably improve a little bit the results.\n",
      "\n",
      "On the other hand the Random Forest took:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rfc = RandomForestClassifier(n_estimators=50)\n",
      "%timeit rfc.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 7.44 s per loop\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Just 7.38 seconds definitely an improvement and the accuracy of both algorithms was very similar on the validation dataset, so Random Forest is a winner on this round."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I am not completlly sure if the delay is because of my implementation or python. Probably a combination of both but my guess is that my implementation has some of the fault. But honestly I don't think that a lot, the Matlab implementation I did (and compare to some I found online) is pretty vectorized and I don't see any obvious mistake on it. I honestly believe that Neural Networks just take longer to train. But as it should be obvious from this post I am not an expert."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Probably the next step is to generallize the implementation to a different number of hidden layers and test the algorith in another dataset, both iris and digit recognition are pretty standard. \n",
      "\n",
      "Also to try a tool like [numba](http://numba.pydata.org) to see if it speeds up the code with some magic. I don't have a lot of expectation because of the vectorized implementation I did should be pretty optimized via numpy, but who knows."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a final conclusion I was pretty amazed by the results, a really simple implementation got some pretty decent results.\n",
      "\n",
      "I was also amazed by the state of the scipy library, the [optimization routines](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) are state of the are from my point of view, amazing work."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}